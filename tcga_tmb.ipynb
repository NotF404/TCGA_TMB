{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# sys.path.append(r\"/mnt/dev/chexpert/CheXpert_Baseline_PyTorch/\")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from torchvision.models import *\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from efficientnet_pytorch import model as enet\n",
    "from pretrainedmodels import se_resnext101_32x4d, se_resnext50_32x4d, densenet121\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "label_mat=3 \n",
    "net_outdim = 1\n",
    "bigpic=True\n",
    "case = f'tmb_fold_batch_size_{batch_size}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset_tmb import TCGADataset\n",
    "dataset_train = TCGADataset(valid_fold=0, tmb_slice=['1','2'], patch_mode=True, expand_mod=False)\n",
    "dl_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True,pin_memory=False)\n",
    "dataset_val = TCGADataset(is_train=False, valid_fold=0, tmb_slice=['1','2'], patch_mode=True, expand_mod=False)\n",
    "dl_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=4,pin_memory=False,drop_last=False)\n",
    "\n",
    "dataset_distile = TCGADataset(expand_mod=True)\n",
    "dl_distile = DataLoader(dataset_distile, batch_size=batch_size*2, shuffle=False, num_workers=0, drop_last=False,pin_memory=False)\n",
    "\n",
    "data = DataBunch(train_dl=dl_train, valid_dl=dl_val, test_dl=dl_distile, device='cuda', no_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class enetv2(nn.Module):\n",
    "    def __init__(self, backbone, out_dim, patch_num, batch_size):\n",
    "        super(enetv2, self).__init__()\n",
    "        self.enet = enet.EfficientNet.from_pretrained(backbone)\n",
    "        self.last_channel_size = self.enet._fc.in_features\n",
    "        self.myfc = nn.Linear(self.last_channel_size, out_dim)\n",
    "        del self.enet._fc\n",
    "        self.patch_num = patch_num\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs=x.shape[0]\n",
    "#         x = x.reshape((bs*self.patch_num, 3, self.img_size, self.img_size))\n",
    "#         print(x.shape)\n",
    "        x = self.enet.extract_features(x)\n",
    "        x = self.enet._avg_pooling(x)\n",
    "#         print(x.shape)\n",
    "        x = x.view(bs, self.last_channel_size)\n",
    "        x = self.enet._dropout(x)\n",
    "#         x = x.reshape((bs, self.last_channel_size))\n",
    "        x = self.myfc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "enet_type = 'efficientnet-b0'\n",
    "model = enetv2(enet_type, net_outdim, 16, batch_size).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnCallbacks(LearnerCallback):\n",
    "    _order = 99\n",
    "    def __init__(self, learn, save_case=''):\n",
    "        super().__init__(learn)\n",
    "        self.epoch = 0\n",
    "        self.skip = False\n",
    "        self.skip_validate = False\n",
    "        self.state = {\"skip_validate\":False}\n",
    "        self.learn = learn\n",
    "        self.save_case = save_case\n",
    "\n",
    "    def on_train_begin(self,**kwargs):\n",
    "        pass\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        print(f'lr_{self.learn.opt.lr}\\twd_{self.learn.opt.wd}')\n",
    "\n",
    "    def on_epoch_end(self,n_epochs,**kwargs):\n",
    "        self.epoch += 1\n",
    "#         self.skip_val(n_epochs)\n",
    "        self.save()\n",
    "        return self.state\n",
    "    \n",
    "    def skip_val(self, n_epochs):\n",
    "        self.skip_validate = ~self.skip_validate\n",
    "        if self.epoch+1 == n_epochs:self.skip_validate=False\n",
    "        self.state.update({\"skip_validate\":self.skip_validate})\n",
    "        \n",
    "    def save(self):\n",
    "        try:\n",
    "            n=self.learn.recorder.metrics[-1][-1].item()\n",
    "        except:\n",
    "            n=0.0\n",
    "        learn.save(f'{self.save_case} {datetime.datetime.now():%Y-%m-%d %H:%M} metrics:{n:.3}')\n",
    "        print(f\"model saved: {self.save_case} {datetime.datetime.now():%Y-%m-%d %H:%M} metrics:{n:.3}\")\n",
    "    \n",
    "    def save_info(self):\n",
    "        pass\n",
    "class acc_ad(Callback):\n",
    "    pass\n",
    "class acc_sc(Callback):\n",
    "    pass\n",
    "class acc_all(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esp = 1e-8\n",
    "\n",
    "    def on_epoch_begin(self,**kwargs):\n",
    "        self.x_t = []\n",
    "        self.y_t = []\n",
    "        self.log_loss = [] \n",
    "        \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        self.x_t.append(last_output.detach().flatten())#.sigmoid()\n",
    "        self.y_t.append(last_target.flatten())  \n",
    "#         pred = logits.sigmoid().sum(1).detach().round()\n",
    "#         print(self.xo, self.yo)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, last_metrics, **kwargs):        \n",
    "        self.last_metrics = last_metrics\n",
    "        self.y_tmb = torch.cat(self.x_t).detach().cpu()\n",
    "        self.l_tmb = torch.cat(self.y_t).detach().cpu()\n",
    "        f1_tmb = mean_absolute_error(self.l_tmb[self.l_tmb>0], self.y_tmb[self.l_tmb>0])\n",
    "        print(f1_tmb)\n",
    "#         print(self.y_tmb, self.l_tmb)\n",
    "        f1_tmb = roc_auc_score(self.l_tmb>0, self.y_tmb)\n",
    "#         print((self.y_tmb>=0).sum(), (self.l_tmb>0).sum())\n",
    "        print(f'tmb mae:{f1_tmb}')\n",
    "        \n",
    "\n",
    "        return {\"last_metrics\":self.last_metrics + [f1_tmb]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, model, loss_func=nn.SmoothL1Loss(), metrics=[acc_all()])\n",
    "learn.callbacks=[LearnCallbacks(learn, case)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics=[acc_all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc_all</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.126361</td>\n",
       "      <td>0.160123</td>\n",
       "      <td>0.564008</td>\n",
       "      <td>02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.140742</td>\n",
       "      <td>0.154847</td>\n",
       "      <td>0.565737</td>\n",
       "      <td>02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137070</td>\n",
       "      <td>0.159931</td>\n",
       "      <td>0.562406</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.146474</td>\n",
       "      <td>0.156272</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158674</td>\n",
       "      <td>0.157169</td>\n",
       "      <td>0.559138</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.153612</td>\n",
       "      <td>0.162277</td>\n",
       "      <td>0.550339</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.131164</td>\n",
       "      <td>0.159572</td>\n",
       "      <td>0.591315</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.143125</td>\n",
       "      <td>0.162633</td>\n",
       "      <td>0.536608</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.149033</td>\n",
       "      <td>0.166865</td>\n",
       "      <td>0.580631</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.147575</td>\n",
       "      <td>0.161163</td>\n",
       "      <td>0.539153</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.156120</td>\n",
       "      <td>0.153127</td>\n",
       "      <td>0.564888</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.132375</td>\n",
       "      <td>0.170080</td>\n",
       "      <td>0.551691</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.161812</td>\n",
       "      <td>0.160238</td>\n",
       "      <td>0.612179</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.156390</td>\n",
       "      <td>0.160826</td>\n",
       "      <td>0.569130</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.131747</td>\n",
       "      <td>0.159931</td>\n",
       "      <td>0.567905</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.144808</td>\n",
       "      <td>0.179707</td>\n",
       "      <td>0.582799</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.129854</td>\n",
       "      <td>0.179978</td>\n",
       "      <td>0.596499</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.117988</td>\n",
       "      <td>0.154885</td>\n",
       "      <td>0.608503</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.142371</td>\n",
       "      <td>0.172751</td>\n",
       "      <td>0.604387</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.131753</td>\n",
       "      <td>0.179641</td>\n",
       "      <td>0.581008</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.136499</td>\n",
       "      <td>0.178287</td>\n",
       "      <td>0.600082</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.128978</td>\n",
       "      <td>0.179468</td>\n",
       "      <td>0.584904</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.135754</td>\n",
       "      <td>0.178270</td>\n",
       "      <td>0.612525</td>\n",
       "      <td>02:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.132089</td>\n",
       "      <td>0.181251</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.114808</td>\n",
       "      <td>0.177262</td>\n",
       "      <td>0.598259</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.112170</td>\n",
       "      <td>0.186345</td>\n",
       "      <td>0.570356</td>\n",
       "      <td>02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.113448</td>\n",
       "      <td>0.184721</td>\n",
       "      <td>0.593766</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.121761</td>\n",
       "      <td>0.182120</td>\n",
       "      <td>0.549899</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.122723</td>\n",
       "      <td>0.187544</td>\n",
       "      <td>0.592949</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.110472</td>\n",
       "      <td>0.185958</td>\n",
       "      <td>0.592226</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_4e-06\twd_0.01\n",
      "1.0008072\n",
      "tmb mae:0.5640082956259427\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 22:48 metrics:0.564\n",
      "lr_6.894754202276402e-06\twd_0.01\n",
      "1.0107304\n",
      "tmb mae:0.5657365510306687\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 22:50 metrics:0.566\n",
      "lr_1.5229866730289061e-05\twd_0.01\n",
      "1.0206956\n",
      "tmb mae:0.5624057315233786\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 22:52 metrics:0.562\n",
      "lr_2.8000000000000003e-05\twd_0.01\n",
      "1.0595853\n",
      "tmb mae:0.548076923076923\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 22:55 metrics:0.548\n",
      "lr_4.366488747198734e-05\twd_0.01\n",
      "0.96322906\n",
      "tmb mae:0.5591377576671694\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 22:57 metrics:0.559\n",
      "lr_6.0335112528012655e-05\twd_0.01\n",
      "1.0666593\n",
      "tmb mae:0.550339366515837\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 22:59 metrics:0.55\n",
      "lr_7.599999999999999e-05\twd_0.01\n",
      "1.0190051\n",
      "tmb mae:0.5913147310206134\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:01 metrics:0.591\n",
      "lr_8.877013326971095e-05\twd_0.01\n",
      "1.0634651\n",
      "tmb mae:0.5366075917546506\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:03 metrics:0.537\n",
      "lr_9.71052457977236e-05\twd_0.01\n",
      "0.98356503\n",
      "tmb mae:0.5806309703368526\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:05 metrics:0.581\n",
      "lr_0.0001\twd_0.01\n",
      "0.9890127\n",
      "tmb mae:0.5391528406234288\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:07 metrics:0.539\n",
      "lr_9.944154354509118e-05\twd_0.01\n",
      "1.0093029\n",
      "tmb mae:0.5648881347410759\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:09 metrics:0.565\n",
      "lr_9.777864917474589e-05\twd_0.01\n",
      "1.0586655\n",
      "tmb mae:0.5516905480140774\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:11 metrics:0.552\n",
      "lr_9.504846320134737e-05\twd_0.01\n",
      "0.9547586\n",
      "tmb mae:0.6121794871794872\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:13 metrics:0.612\n",
      "lr_9.131197346804488e-05\twd_0.01\n",
      "1.0348046\n",
      "tmb mae:0.5691302161890397\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:15 metrics:0.569\n",
      "lr_8.665264698111695e-05\twd_0.01\n",
      "1.0481695\n",
      "tmb mae:0.5679047259929613\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:17 metrics:0.568\n",
      "lr_8.117456539497631e-05\twd_0.01\n",
      "1.0061466\n",
      "tmb mae:0.5827991452991453\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:20 metrics:0.583\n",
      "lr_7.50001e-05\twd_0.01\n",
      "1.032559\n",
      "tmb mae:0.5964994972347913\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:22 metrics:0.596\n",
      "lr_6.826717815011489e-05\twd_0.01\n",
      "1.0272777\n",
      "tmb mae:0.6085030165912519\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:24 metrics:0.609\n",
      "lr_6.112620219362893e-05\twd_0.01\n",
      "0.95206803\n",
      "tmb mae:0.6043866264454499\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:26 metrics:0.604\n",
      "lr_5.3736689733302504e-05\twd_0.01\n",
      "1.0096706\n",
      "tmb mae:0.5810080442433384\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:28 metrics:0.581\n",
      "lr_4.626371026669751e-05\twd_0.01\n",
      "1.0000563\n",
      "tmb mae:0.6000816993464053\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:30 metrics:0.6\n",
      "lr_3.8874197806371076e-05\twd_0.01\n",
      "0.9891873\n",
      "tmb mae:0.5849044746103569\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:32 metrics:0.585\n",
      "lr_3.1733221849885125e-05\twd_0.01\n",
      "0.959124\n",
      "tmb mae:0.6125251382604324\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:34 metrics:0.613\n",
      "lr_2.5000300000000012e-05\twd_0.01\n",
      "1.0119288\n",
      "tmb mae:0.5674962292609351\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:36 metrics:0.567\n",
      "lr_1.88258346050237e-05\twd_0.01\n",
      "1.0182735\n",
      "tmb mae:0.5982591754650578\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:39 metrics:0.598\n",
      "lr_1.334775301888306e-05\twd_0.01\n",
      "1.0546242\n",
      "tmb mae:0.5703557063851181\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:41 metrics:0.57\n",
      "lr_8.688426531955128e-06\twd_0.01\n",
      "1.0435721\n",
      "tmb mae:0.5937657114127702\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:43 metrics:0.594\n",
      "lr_4.9519367986526286e-06\twd_0.01\n",
      "1.040255\n",
      "tmb mae:0.5498994469582705\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:45 metrics:0.55\n",
      "lr_2.221750825254118e-06\twd_0.01\n",
      "1.080527\n",
      "tmb mae:0.592948717948718\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:47 metrics:0.593\n",
      "lr_5.588564549088189e-07\twd_0.01\n",
      "1.0641608\n",
      "tmb mae:0.5922259929612871\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 23:49 metrics:0.592\n"
     ]
    }
   ],
   "source": [
    "lr =1e-4\n",
    "learn.fit_one_cycle(30,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc_all</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.123084</td>\n",
       "      <td>0.182711</td>\n",
       "      <td>0.578494</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.125820</td>\n",
       "      <td>0.170253</td>\n",
       "      <td>0.566114</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.132300</td>\n",
       "      <td>0.215616</td>\n",
       "      <td>0.584873</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.138920</td>\n",
       "      <td>0.166228</td>\n",
       "      <td>0.557190</td>\n",
       "      <td>02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.168905</td>\n",
       "      <td>0.208532</td>\n",
       "      <td>0.423423</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.152864</td>\n",
       "      <td>0.150356</td>\n",
       "      <td>0.501225</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.163718</td>\n",
       "      <td>0.148282</td>\n",
       "      <td>0.604858</td>\n",
       "      <td>02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.170887</td>\n",
       "      <td>0.148191</td>\n",
       "      <td>0.468043</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.184885</td>\n",
       "      <td>0.166862</td>\n",
       "      <td>0.552696</td>\n",
       "      <td>02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.176346</td>\n",
       "      <td>0.153737</td>\n",
       "      <td>0.438631</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.147429</td>\n",
       "      <td>0.432064</td>\n",
       "      <td>02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.176399</td>\n",
       "      <td>0.147665</td>\n",
       "      <td>0.504996</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.158157</td>\n",
       "      <td>0.144085</td>\n",
       "      <td>0.434986</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.164437</td>\n",
       "      <td>0.146813</td>\n",
       "      <td>0.519796</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.174884</td>\n",
       "      <td>0.145217</td>\n",
       "      <td>0.511375</td>\n",
       "      <td>02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.181031</td>\n",
       "      <td>0.142325</td>\n",
       "      <td>0.496638</td>\n",
       "      <td>02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.158848</td>\n",
       "      <td>0.143090</td>\n",
       "      <td>0.498146</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.178485</td>\n",
       "      <td>0.140297</td>\n",
       "      <td>0.488751</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.156023</td>\n",
       "      <td>0.142229</td>\n",
       "      <td>0.495978</td>\n",
       "      <td>02:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.148199</td>\n",
       "      <td>0.140434</td>\n",
       "      <td>0.495318</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.151464</td>\n",
       "      <td>0.157421</td>\n",
       "      <td>0.476401</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.139541</td>\n",
       "      <td>0.141117</td>\n",
       "      <td>0.509207</td>\n",
       "      <td>02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.181462</td>\n",
       "      <td>0.143234</td>\n",
       "      <td>0.532994</td>\n",
       "      <td>02:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.169065</td>\n",
       "      <td>0.141396</td>\n",
       "      <td>0.535288</td>\n",
       "      <td>02:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.168013</td>\n",
       "      <td>0.143590</td>\n",
       "      <td>0.516937</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.163332</td>\n",
       "      <td>0.142672</td>\n",
       "      <td>0.525327</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.164644</td>\n",
       "      <td>0.140785</td>\n",
       "      <td>0.559043</td>\n",
       "      <td>02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.150496</td>\n",
       "      <td>0.140550</td>\n",
       "      <td>0.556561</td>\n",
       "      <td>02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.159462</td>\n",
       "      <td>0.139236</td>\n",
       "      <td>0.569853</td>\n",
       "      <td>02:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.142408</td>\n",
       "      <td>0.140977</td>\n",
       "      <td>0.553859</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_4e-05\twd_0.01\n",
      "1.0383235\n",
      "tmb mae:0.5784942182001006\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:26 metrics:0.578\n",
      "lr_6.8947542022764e-05\twd_0.01\n",
      "1.02005\n",
      "tmb mae:0.5661136249371543\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:28 metrics:0.566\n",
      "lr_0.0001522986673028906\twd_0.01\n",
      "0.9063783\n",
      "tmb mae:0.5848730517848165\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:30 metrics:0.585\n",
      "lr_0.00028\twd_0.01\n",
      "0.97895247\n",
      "tmb mae:0.5571895424836601\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:32 metrics:0.557\n",
      "lr_0.00043664887471987334\twd_0.01\n",
      "1.4193715\n",
      "tmb mae:0.4234225741578683\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:34 metrics:0.423\n",
      "lr_0.0006033511252801266\twd_0.01\n",
      "1.1270839\n",
      "tmb mae:0.5012254901960784\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:36 metrics:0.501\n",
      "lr_0.0007599999999999999\twd_0.01\n",
      "1.1310614\n",
      "tmb mae:0.604857968828557\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:38 metrics:0.605\n",
      "lr_0.0008877013326971094\twd_0.01\n",
      "1.1102203\n",
      "tmb mae:0.46804298642533937\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:40 metrics:0.468\n",
      "lr_0.000971052457977236\twd_0.01\n",
      "1.2177262\n",
      "tmb mae:0.5526960784313726\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:42 metrics:0.553\n",
      "lr_0.001\twd_0.01\n",
      "0.95876205\n",
      "tmb mae:0.43863122171945707\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:44 metrics:0.439\n",
      "lr_0.0009944154354509117\twd_0.01\n",
      "1.1576521\n",
      "tmb mae:0.43206385118149826\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:46 metrics:0.432\n",
      "lr_0.0009777864917474587\twd_0.01\n",
      "1.0978379\n",
      "tmb mae:0.5049962292609351\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:48 metrics:0.505\n",
      "lr_0.0009504846320134736\twd_0.01\n",
      "1.1295165\n",
      "tmb mae:0.4349861739567622\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:50 metrics:0.435\n",
      "lr_0.0009131197346804487\twd_0.01\n",
      "0.9684817\n",
      "tmb mae:0.5197963800904977\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:53 metrics:0.52\n",
      "lr_0.0008665264698111694\twd_0.01\n",
      "1.0807217\n",
      "tmb mae:0.511375062845651\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:55 metrics:0.511\n",
      "lr_0.000811745653949763\twd_0.01\n",
      "1.0335748\n",
      "tmb mae:0.49663775766716944\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:57 metrics:0.497\n",
      "lr_0.0007500009999999999\twd_0.01\n",
      "1.1222631\n",
      "tmb mae:0.4981460532931121\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 10:59 metrics:0.498\n",
      "lr_0.0006826717815011488\twd_0.01\n",
      "1.0690155\n",
      "tmb mae:0.48875062845651085\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:01 metrics:0.489\n",
      "lr_0.0006112620219362892\twd_0.01\n",
      "1.0160505\n",
      "tmb mae:0.4959778783308195\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:03 metrics:0.496\n",
      "lr_0.0005373668973330249\twd_0.01\n",
      "1.0932887\n",
      "tmb mae:0.4953179989944696\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:05 metrics:0.495\n",
      "lr_0.00046263710266697503\twd_0.01\n",
      "1.2041923\n",
      "tmb mae:0.47640145801910505\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:07 metrics:0.476\n",
      "lr_0.00038874197806371076\twd_0.01\n",
      "1.086956\n",
      "tmb mae:0.5092068878833584\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:09 metrics:0.509\n",
      "lr_0.0003173322184988512\twd_0.01\n",
      "1.0997154\n",
      "tmb mae:0.5329939668174962\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:12 metrics:0.533\n",
      "lr_0.0002500030000000001\twd_0.01\n",
      "1.0654159\n",
      "tmb mae:0.5352878330819508\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:14 metrics:0.535\n",
      "lr_0.00018825834605023698\twd_0.01\n",
      "1.1056892\n",
      "tmb mae:0.5169369029663147\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:16 metrics:0.517\n",
      "lr_0.0001334775301888306\twd_0.01\n",
      "1.061875\n",
      "tmb mae:0.5253267973856209\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:18 metrics:0.525\n",
      "lr_8.688426531955129e-05\twd_0.01\n",
      "1.072544\n",
      "tmb mae:0.559043489190548\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:20 metrics:0.559\n",
      "lr_4.9519367986526286e-05\twd_0.01\n",
      "1.0691419\n",
      "tmb mae:0.5565610859728507\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:22 metrics:0.557\n",
      "lr_2.2217508252541176e-05\twd_0.01\n",
      "1.0545552\n",
      "tmb mae:0.5698529411764706\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:24 metrics:0.57\n",
      "lr_5.588564549088189e-06\twd_0.01\n",
      "1.0617323\n",
      "tmb mae:0.5538587229763701\n",
      "model saved: tmb_fold_batch_size_8 2020-07-01 11:26 metrics:0.554\n"
     ]
    }
   ],
   "source": [
    "lr =1e-3\n",
    "learn.fit_one_cycle(30,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-5.1393e-01, -1.1359e+00, -5.3170e-01, -6.0351e-01, -2.2838e-01,\n",
      "        -1.8872e-01, -3.4723e-01, -3.0909e-01, -4.1755e-01, -3.1941e-01,\n",
      "         1.4670e-02, -7.6058e-02, -1.8347e-01, -4.8396e-01, -4.7792e-01,\n",
      "        -6.7216e-01, -3.3592e-01, -5.9198e-01, -3.9655e-01, -2.8151e-01,\n",
      "        -3.3516e-01, -2.2147e-01, -3.5040e-01, -5.4543e-01, -2.0055e-01,\n",
      "        -5.1220e-01, -2.8122e-01, -3.1906e-01, -3.3232e-01, -7.2321e-02,\n",
      "        -1.1532e-01, -8.1220e-01, -3.3024e-01, -3.0315e-01, -1.6136e-01,\n",
      "        -4.2075e-01, -2.7983e-01, -9.0099e-01, -2.3155e-01, -5.9525e-01,\n",
      "        -2.8585e-01,  1.3763e-01, -6.7996e-01, -3.8177e-01, -6.9915e-01,\n",
      "        -6.8695e-01, -3.7221e-01, -5.3371e-01, -2.8220e-01, -1.7869e-01,\n",
      "        -2.3581e-01, -4.3992e-01, -2.9468e-01, -7.2156e-01, -1.3517e-01,\n",
      "        -5.7974e-01, -1.4961e-01, -4.3119e-01, -4.5308e-01, -5.8438e-01,\n",
      "        -7.2349e-01, -4.9823e-01, -6.5122e-01, -4.2443e-01, -5.6604e-01,\n",
      "        -4.4913e-02, -2.6458e-01, -5.5820e-01, -6.0579e-01,  2.6797e-01,\n",
      "        -6.2379e-01,  1.3905e-01, -3.3374e-01,  8.1102e-02, -6.0730e-01,\n",
      "        -3.6495e-01, -6.3938e-01, -9.6011e-01, -3.9892e-01, -6.2336e-01,\n",
      "        -4.0809e-01, -5.3482e-01, -1.1520e-02, -5.3651e-01, -8.6210e-01,\n",
      "        -5.6619e-01, -1.7620e-01, -5.1460e-01, -3.5588e-01, -4.6445e-01,\n",
      "        -6.4589e-01, -2.7698e-01, -7.2063e-01, -7.3075e-01, -5.7413e-01,\n",
      "        -2.4211e-01, -4.8860e-01, -5.2718e-01, -2.2102e-01, -2.7443e-01,\n",
      "        -5.2933e-01, -4.0215e-01, -4.2581e-01, -2.6024e-01, -4.2900e-01,\n",
      "        -3.3066e-01, -3.1711e-01,  1.5759e-01, -3.5245e-01, -2.8472e-01,\n",
      "        -3.8159e-01, -1.8450e-01, -3.4161e-01, -4.3184e-01, -5.5347e-01,\n",
      "        -5.7650e-01, -1.8728e-01, -1.4343e-01, -2.9554e-01, -2.0171e-01,\n",
      "        -6.8055e-01, -4.2354e-01, -3.5118e-01, -2.9833e-01, -1.0012e+00,\n",
      "        -3.1007e-01, -3.1346e-01, -4.3459e-01, -4.1224e-01, -5.0149e-01,\n",
      "        -1.3784e-01, -6.2800e-01, -1.2344e-01, -8.2044e-01, -2.2698e-01,\n",
      "        -6.5669e-01, -4.3656e-01, -5.2061e-01, -5.5280e-01, -4.0758e-01,\n",
      "        -3.6514e-01, -3.2018e-01, -1.7592e-01, -3.6090e-01, -2.4957e-01,\n",
      "        -1.1211e-01, -2.4211e-01, -1.6000e-01, -3.3408e-01, -4.3037e-01,\n",
      "        -9.8174e-02, -6.0384e-01, -1.0303e-01, -1.6401e-01, -2.7808e-01,\n",
      "        -1.9531e-01, -4.4823e-01, -1.1005e-01, -1.1576e-01, -2.0022e-01,\n",
      "         7.7637e-04,  2.7735e-02, -2.2423e-01, -2.7867e-01, -5.9351e-01,\n",
      "        -6.8828e-01, -2.5583e-01, -1.9703e-01, -7.4458e-02, -6.8110e-01,\n",
      "        -3.4804e-01, -4.1676e-01, -6.5897e-01, -4.6495e-01, -1.3855e-01,\n",
      "        -5.5507e-01, -5.2117e-01, -4.8574e-01, -4.8477e-01, -2.8977e-01,\n",
      "        -3.5045e-01, -5.3323e-01, -1.9574e-01, -9.6107e-02, -4.2137e-01,\n",
      "        -7.5860e-02, -4.0936e-01, -4.8747e-01, -1.5832e-01, -5.1237e-01,\n",
      "        -4.3251e-02, -4.0671e-01, -2.8001e-01, -2.0401e-01, -3.8373e-01,\n",
      "        -2.7016e-01, -5.9308e-01, -2.7895e-01, -6.5495e-01, -3.4590e-01,\n",
      "        -2.7868e-01, -3.5696e-01, -3.4547e-01, -4.6988e-01,  4.9492e-02,\n",
      "        -6.6255e-01,  9.9252e-02,  2.9813e-01, -4.5902e-01, -3.3946e-01,\n",
      "        -2.8098e-01, -3.0398e-01,  9.4953e-02, -3.1871e-01, -1.7421e-01,\n",
      "         5.9627e-02, -7.2350e-01, -5.8464e-02, -5.8626e-01, -2.3920e-01,\n",
      "        -3.1836e-02, -5.4924e-01, -4.6825e-01, -5.2310e-01, -5.7465e-01,\n",
      "        -5.2153e-01, -6.6450e-01, -5.4706e-01, -4.1986e-01, -1.5198e-01,\n",
      "        -2.4907e-01, -2.7487e-02, -2.0264e-01, -5.0064e-01, -1.6922e-01,\n",
      "        -6.7019e-01, -4.0121e-01, -4.0380e-01, -5.4674e-01, -2.4104e-02,\n",
      "        -5.6277e-01, -1.5647e-01, -1.8108e-01, -2.6728e-02, -4.9736e-01,\n",
      "        -3.7541e-01, -3.8117e-01, -3.1662e-01, -4.5829e-01, -2.6965e-01,\n",
      "        -3.4536e-01, -3.3369e-01, -6.5985e-01, -5.5686e-01, -4.0221e-01,\n",
      "        -2.6446e-02, -5.5665e-02, -3.1373e-01, -2.7912e-01, -4.6979e-01,\n",
      "        -5.1676e-01, -5.6597e-01, -2.8213e-01, -5.7414e-01, -5.0754e-01,\n",
      "        -5.3721e-01, -7.0964e-01,  1.2464e-01,  3.6506e-01, -5.0053e-01,\n",
      "        -3.7399e-01, -5.3816e-02, -4.0486e-01, -4.5710e-01, -4.4299e-01,\n",
      "        -5.9726e-01, -6.9691e-01, -4.8070e-01, -5.0716e-01, -3.6928e-01,\n",
      "        -5.5220e-01, -3.1460e-01, -4.2415e-01, -3.1589e-01, -2.8875e-01,\n",
      "        -2.4950e-01, -1.7437e-01, -3.9813e-01, -2.4393e-01, -3.8199e-01,\n",
      "        -5.2223e-01, -4.3271e-01, -6.7272e-02, -6.7454e-01, -5.7370e-01,\n",
      "        -2.7307e-01, -4.5765e-01, -8.5560e-01, -5.4708e-01, -2.5627e-01,\n",
      "        -2.4681e-01, -5.0289e-01, -2.1266e-01, -2.6997e-01, -2.1939e-01,\n",
      "        -3.3188e-01, -1.3672e-01, -5.5277e-01, -5.3393e-01, -7.4144e-01,\n",
      "        -5.0059e-01, -6.5433e-01, -2.9188e-01, -8.3840e-02, -7.8414e-02,\n",
      "        -5.9287e-01, -5.1939e-01, -4.9154e-01, -1.1359e-01, -3.4127e-01,\n",
      "        -6.3535e-01, -2.2001e-01, -2.3913e-01, -3.0332e-01, -8.3848e-02,\n",
      "        -5.9539e-01,  3.7510e-02, -1.0155e+00, -6.3155e-01,  4.4456e-02,\n",
      "        -3.2779e-01, -6.2174e-01, -8.4189e-01, -2.9966e-01, -8.6994e-01,\n",
      "         1.2220e-01, -2.4263e-01, -5.1225e-01, -4.8191e-01, -3.4191e-01,\n",
      "        -2.8112e-01, -2.3652e-02, -6.7652e-01, -7.5773e-01, -3.4024e-01,\n",
      "        -7.0879e-01, -7.4881e-01,  6.6465e-01, -1.8094e-01, -1.4515e-01,\n",
      "        -3.8140e-01, -4.2003e-01, -5.5601e-01, -6.3631e-01, -4.4021e-01,\n",
      "         1.0929e-01, -3.7586e-01, -2.4607e-01, -5.9199e-01, -3.1919e-01,\n",
      "        -4.0412e-01, -2.7965e-01, -2.6386e-01, -1.6003e-01, -5.5711e-01,\n",
      "        -6.1582e-01, -8.5015e-02, -4.9571e-01, -3.4360e-01, -4.6832e-01,\n",
      "        -1.9500e-01, -2.3360e-01, -4.8409e-01, -2.2379e-01, -2.2926e-01,\n",
      "        -3.1363e-01, -1.6780e-01, -3.0770e-01, -6.5178e-01, -2.7098e-01,\n",
      "        -4.8056e-01, -5.5982e-01, -1.2563e-01, -1.9058e-01, -2.4514e-01,\n",
      "        -4.8191e-01,  4.0228e-02, -6.9293e-01,  9.6627e-02, -4.9030e-02,\n",
      "        -3.3847e-01, -1.7978e-01,  3.9428e-02, -4.3565e-02,  6.1689e-02,\n",
      "        -8.1403e-02, -3.5253e-01, -3.5724e-01, -4.2997e-01, -3.0666e-01,\n",
      "        -1.6636e-01, -4.9763e-01, -3.9210e-01, -6.7521e-01, -1.8963e-01,\n",
      "        -3.7819e-01, -6.4690e-01, -2.4654e-01, -6.6863e-01, -6.8649e-01,\n",
      "        -5.6035e-01, -2.2554e-01, -2.1474e-01, -2.2274e-01, -9.2727e-03,\n",
      "        -5.8369e-01, -2.0165e-01,  2.6960e-02, -2.2295e-01, -3.7242e-02,\n",
      "        -3.1430e-01, -1.8714e-01, -1.8479e-01,  2.5023e-02, -2.7982e-01,\n",
      "        -3.0549e-01, -4.8208e-01, -6.4888e-02,  1.0581e-01, -5.3403e-01,\n",
      "        -2.8650e-01, -2.5582e-01, -6.0520e-01, -2.4453e-01, -3.9304e-01,\n",
      "        -3.7717e-01, -5.2192e-01, -6.9136e-02, -5.8311e-01,  4.0750e-02,\n",
      "        -2.5217e-01, -3.1500e-01, -9.4043e-02, -3.9181e-01, -4.2355e-01,\n",
      "        -6.4015e-01, -2.8011e-01, -1.7541e-01,  1.1412e-01,  4.4106e-02,\n",
      "         3.3000e-01, -6.1997e-01, -2.5781e-01, -5.3697e-01, -6.4086e-01,\n",
      "        -6.2769e-01,  7.2510e-02, -2.0936e-01, -2.5749e-01, -1.4054e-01,\n",
      "        -6.1090e-01, -3.0189e-01, -2.7507e-01, -9.8030e-02, -2.2119e-01,\n",
      "        -2.7868e-01, -3.2630e-01, -7.6657e-01, -5.0062e-01, -3.5831e-01,\n",
      "        -3.2790e-01, -1.0765e+00, -1.4561e-01, -1.5178e-01, -1.3925e-01,\n",
      "        -7.0081e-02,  6.2806e-03,  2.3980e-01, -5.5271e-01, -6.7157e-01,\n",
      "        -2.3012e-01, -5.4091e-01, -1.6142e-01, -2.6064e-01, -2.6350e-01,\n",
      "        -5.7371e-01]) tensor([-6.1053e-01, -6.1053e-01, -9.2368e-01, -9.2368e-01, -1.3947e-01,\n",
      "        -1.3947e-01, -1.7105e-01, -1.7105e-01,  3.7368e-01,  3.7368e-01,\n",
      "        -4.5789e-01, -4.5789e-01, -3.1579e-01, -3.1579e-01, -4.4737e-02,\n",
      "        -4.4737e-02, -3.1579e-01, -3.1579e-01, -8.6316e-01, -8.6316e-01,\n",
      "        -7.3684e-01, -7.3684e-01, -9.0263e-01, -9.0263e-01,  8.1579e-02,\n",
      "         8.1579e-02,  1.9500e+00,  1.9500e+00,  1.9500e+00,  7.2105e-01,\n",
      "         7.2105e-01, -1.8684e-01, -1.3158e-01, -5.8158e-01, -5.8158e-01,\n",
      "        -3.7368e-01, -3.7368e-01, -3.7368e-01, -6.7632e-01, -9.3684e-01,\n",
      "        -9.3684e-01, -8.2632e-01, -8.2632e-01, -8.1579e-02, -8.1579e-02,\n",
      "        -7.4474e-01,  1.7368e-01,  1.7368e-01, -4.5000e-01, -4.5000e-01,\n",
      "         1.7237e+00,  1.7237e+00, -6.0000e-01, -6.0000e-01, -4.2105e-02,\n",
      "        -4.2105e-02,  8.9474e-02,  8.9474e-02, -5.8947e-01, -5.8947e-01,\n",
      "        -3.5000e-01,  1.3684e-01,  1.3684e-01, -8.2368e-01, -8.2368e-01,\n",
      "        -8.4211e-02, -8.4211e-02, -9.3947e-01, -9.3947e-01, -1.6842e-01,\n",
      "        -9.5263e-01, -5.1053e-01, -7.1579e-01, -3.2368e-01, -7.2105e-01,\n",
      "        -9.1579e-01, -9.1579e-01, -9.1053e-01, -6.2895e-01,  1.4211e-01,\n",
      "        -1.0000e-01, -9.2368e-01, -9.2368e-01, -4.3158e-01, -4.3158e-01,\n",
      "        -9.4211e-01, -9.4211e-01, -4.4474e-01, -4.4474e-01, -2.5789e-01,\n",
      "        -2.5789e-01, -1.6053e-01, -5.9474e-01, -5.9474e-01, -3.0000e-01,\n",
      "        -2.2368e-01, -9.0789e-01, -9.0789e-01, -5.9211e-01, -5.9211e-01,\n",
      "        -2.2895e-01, -3.7895e-01, -3.7895e-01, -2.7368e-01, -2.7368e-01,\n",
      "        -4.3158e-01, -4.3158e-01, -4.6579e-01, -9.2105e-02, -7.0789e-01,\n",
      "        -7.0789e-01, -5.2105e-01, -9.3158e-01, -8.3947e-01, -9.3158e-01,\n",
      "        -5.2368e-01, -5.2368e-01, -5.3684e-01, -5.3684e-01, -5.2632e-01,\n",
      "        -5.2632e-01, -7.9737e-01, -9.3158e-01, -1.4211e-01, -9.3158e-01,\n",
      "        -5.1842e-01, -1.7368e-01, -5.8421e-01, -8.7895e-01, -2.7105e-01,\n",
      "        -2.7105e-01, -3.0000e-01, -1.1842e-01, -3.0000e-01, -5.7632e-01,\n",
      "        -2.2368e-01, -2.2368e-01, -4.5789e-01, -4.5789e-01, -9.1316e-01,\n",
      "        -9.1316e-01, -8.5789e-01, -2.4474e-01, -8.5789e-01, -1.3158e-02,\n",
      "        -1.3158e-02, -7.3947e-01, -7.3947e-01, -2.0526e-01, -2.0526e-01,\n",
      "        -5.6316e-01, -5.6316e-01, -5.6053e-01, -5.6053e-01, -3.7632e-01,\n",
      "        -5.2632e-01, -5.2632e-01, -4.6053e-01,  3.9474e-02, -8.2105e-01,\n",
      "        -8.2105e-01,  1.7632e+00, -4.7105e-01, -4.7105e-01, -7.9737e-01,\n",
      "        -7.9737e-01, -7.3158e-01, -7.3158e-01,  1.7632e-01,  1.7632e-01,\n",
      "        -3.4474e-01, -3.4474e-01, -7.5526e-01, -7.5526e-01, -2.9474e-01,\n",
      "        -2.9474e-01,  1.4447e+00,  1.4447e+00, -2.5263e-01, -2.5263e-01,\n",
      "        -1.2895e-01, -1.2895e-01,  2.0000e-01,  1.4447e+00,  4.7632e-01,\n",
      "        -1.8421e-01, -7.0263e-01, -7.0263e-01, -4.7895e-01, -2.3684e-02,\n",
      "        -1.7368e-01, -2.3684e-02, -5.7895e-01, -4.3421e-01, -4.3421e-01,\n",
      "        -4.3421e-01,  6.8158e-01, -3.2632e-01,  6.8158e-01, -3.2632e-01,\n",
      "         6.8158e-01, -1.9737e-01, -5.7895e-02, -5.7895e-02, -4.3158e-01,\n",
      "        -4.3158e-01,  2.3684e-01,  2.3684e-01, -8.9211e-01, -8.9211e-01,\n",
      "         4.5263e-01,  4.5263e-01, -9.6316e-01, -9.6316e-01, -5.2632e-01,\n",
      "        -5.2632e-01, -9.8947e-01, -5.1842e-01, -5.1842e-01, -2.4211e-01,\n",
      "        -2.9737e-01, -2.4211e-01, -8.8158e-01, -8.8158e-01, -9.1053e-01,\n",
      "        -6.2105e-01, -6.2105e-01, -7.0789e-01, -7.0789e-01, -2.6053e-01,\n",
      "        -2.6053e-01, -4.6579e-01, -4.6579e-01, -7.0526e-01, -7.0526e-01,\n",
      "        -4.8684e-01, -4.8684e-01, -5.0000e-01, -5.0000e-01, -5.3947e-01,\n",
      "        -9.9737e-01, -9.9737e-01, -7.2632e-01, -7.2632e-01, -4.4737e-01,\n",
      "        -5.4211e-01, -6.2105e-01,  2.6316e-02,  2.6316e-02, -5.1842e-01,\n",
      "        -5.9211e-01, -2.6316e-03, -8.4211e-01, -9.9211e-01, -9.9211e-01,\n",
      "         1.1316e-01,  1.1316e-01, -7.2632e-01, -7.2632e-01, -5.1316e-01,\n",
      "        -5.1316e-01, -9.6842e-01, -9.6842e-01, -4.4737e-02, -4.4737e-02,\n",
      "        -7.5526e-01, -7.5526e-01,  2.7605e+00,  2.7605e+00, -5.9474e-01,\n",
      "        -5.9474e-01, -4.9474e-01, -7.6053e-01, -7.5000e-01, -8.1579e-01,\n",
      "        -9.1316e-01, -5.7105e-01, -5.7105e-01,  1.5500e+00,  4.7368e-02,\n",
      "        -1.0789e-01, -1.0789e-01, -3.5789e-01,  5.4211e-01, -5.2368e-01,\n",
      "        -5.2368e-01, -5.3158e-01, -4.2368e-01, -5.3158e-01,  3.7368e-01,\n",
      "        -8.7895e-01,  5.3421e-01,  5.3421e-01, -9.3684e-01, -9.3684e-01,\n",
      "         5.3421e-01, -8.7895e-01, -8.7895e-01,  3.7368e-01,  3.7368e-01,\n",
      "        -5.3158e-01, -4.2368e-01, -5.2368e-01,  5.4211e-01,  5.4211e-01,\n",
      "        -3.5789e-01, -1.0789e-01,  4.7368e-02,  1.5500e+00, -8.1579e-01,\n",
      "        -8.1579e-01, -8.1579e-01, -4.9474e-01, -4.9474e-01,  2.7605e+00,\n",
      "        -8.4211e-01, -8.4211e-01,  6.7368e-01,  6.7368e-01,  9.4737e-02,\n",
      "         9.4737e-02, -5.4211e-01, -4.4737e-01, -9.9737e-01, -5.3947e-01,\n",
      "        -5.3947e-01, -5.3947e-01, -5.0000e-01, -4.8684e-01, -7.0526e-01,\n",
      "        -4.6579e-01, -1.9211e-01, -1.9211e-01, -8.5263e-01, -8.5263e-01,\n",
      "        -8.7368e-01, -8.7368e-01, -5.7632e-01, -5.7632e-01, -2.6053e-01,\n",
      "        -7.0789e-01, -2.9737e-01, -2.4211e-01, -2.9737e-01, -5.1842e-01,\n",
      "        -9.8947e-01, -9.8947e-01,  4.5263e-01, -8.9211e-01,  2.3684e-01,\n",
      "        -4.3158e-01, -5.7895e-02, -1.9737e-01, -1.9737e-01,  6.8158e-01,\n",
      "         6.8158e-01,  6.8158e-01, -3.2632e-01,  6.8158e-01, -4.3421e-01,\n",
      "        -7.1842e-01, -7.1842e-01, -5.7895e-01, -5.7895e-01, -1.7368e-01,\n",
      "        -2.3684e-02, -1.7368e-01, -4.7895e-01, -4.7895e-01, -7.0263e-01,\n",
      "        -1.8421e-01, -1.8421e-01,  4.7632e-01,  4.7632e-01,  1.4447e+00,\n",
      "         1.4447e+00,  2.0000e-01,  2.0000e-01, -1.2895e-01, -2.5263e-01,\n",
      "         1.4447e+00, -7.5526e-01, -7.3158e-01, -7.9737e-01, -4.7105e-01,\n",
      "         1.7632e+00,  1.7632e+00, -8.2105e-01,  3.9474e-02,  3.9474e-02,\n",
      "        -4.6053e-01, -4.6053e-01, -5.2632e-01, -3.7632e-01, -5.6053e-01,\n",
      "        -7.3947e-01, -2.4474e-01, -8.5789e-01, -2.4474e-01, -8.9474e-02,\n",
      "        -8.9474e-02, -7.6316e-02, -7.6316e-02, -2.2368e-01, -5.7632e-01,\n",
      "        -5.7632e-01, -5.7632e-01, -5.7632e-01, -5.7632e-01, -5.7632e-01,\n",
      "        -5.7632e-01, -5.7632e-01, -1.1842e-01, -1.1842e-01, -1.1842e-01,\n",
      "        -3.0000e-01, -1.1842e-01, -1.0789e-01, -1.0789e-01, -4.5263e-01,\n",
      "        -4.5263e-01, -4.7368e-01, -4.7368e-01, -2.9474e-01, -2.9474e-01,\n",
      "        -2.4737e-01, -2.7105e-01, -8.7895e-01, -8.7895e-01, -5.8421e-01,\n",
      "        -5.8421e-01, -1.7368e-01, -1.7368e-01, -5.1842e-01, -5.1842e-01,\n",
      "        -1.4211e-01, -9.3158e-01, -1.4211e-01, -7.9737e-01, -7.9737e-01,\n",
      "        -8.3947e-01, -9.3158e-01, -8.3947e-01, -5.2105e-01, -5.2105e-01,\n",
      "        -7.0789e-01,  1.8421e-02, -9.2105e-02, -9.2105e-02, -4.6579e-01,\n",
      "        -4.6579e-01, -4.3158e-01, -2.7368e-01, -2.2895e-01, -2.2895e-01,\n",
      "        -9.0789e-01, -2.2368e-01, -2.2368e-01, -3.0000e-01, -3.0000e-01,\n",
      "        -5.9474e-01, -1.6053e-01, -1.6053e-01, -4.4474e-01, -9.4211e-01,\n",
      "        -4.3158e-01, -9.2368e-01, -1.0000e-01,  1.4211e-01, -6.2895e-01,\n",
      "        -9.1053e-01, -9.1053e-01, -3.2368e-01, -7.1579e-01, -5.1053e-01,\n",
      "        -5.1053e-01, -9.5263e-01, -1.6842e-01, -3.5000e-01, -3.5000e-01,\n",
      "         1.7237e+00, -7.4474e-01, -1.3158e-01, -1.3158e-01, -1.8684e-01,\n",
      "        -1.8684e-01])\n",
      "tmb mae:0.581856460532931\n",
      "model saved: tmb_fold_batch_size_8 2020-06-30 21:25 metrics:0.527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14996693, 0.581856460532931]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate(metrics=[acc_all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='16' class='' max='30', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      53.33% [16/30 29:06<25:28]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc_all</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.282936</td>\n",
       "      <td>0.703336</td>\n",
       "      <td>0.594614</td>\n",
       "      <td>02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.323030</td>\n",
       "      <td>0.646773</td>\n",
       "      <td>0.635087</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.289501</td>\n",
       "      <td>0.688594</td>\n",
       "      <td>0.573749</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.283212</td>\n",
       "      <td>0.730725</td>\n",
       "      <td>0.572587</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.746019</td>\n",
       "      <td>0.594237</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.269379</td>\n",
       "      <td>0.719967</td>\n",
       "      <td>0.593294</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.250671</td>\n",
       "      <td>0.674989</td>\n",
       "      <td>0.621198</td>\n",
       "      <td>01:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.266391</td>\n",
       "      <td>0.707985</td>\n",
       "      <td>0.605738</td>\n",
       "      <td>01:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.282604</td>\n",
       "      <td>0.603642</td>\n",
       "      <td>0.648159</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.287259</td>\n",
       "      <td>0.601695</td>\n",
       "      <td>0.668583</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.248570</td>\n",
       "      <td>0.654356</td>\n",
       "      <td>0.656423</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.281998</td>\n",
       "      <td>0.655373</td>\n",
       "      <td>0.630310</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.286565</td>\n",
       "      <td>0.628481</td>\n",
       "      <td>0.630153</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.245884</td>\n",
       "      <td>0.670308</td>\n",
       "      <td>0.598731</td>\n",
       "      <td>01:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.234240</td>\n",
       "      <td>0.649469</td>\n",
       "      <td>0.655323</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.256136</td>\n",
       "      <td>0.676699</td>\n",
       "      <td>0.632699</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='81' class='' max='197', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      41.12% [81/197 00:42<01:00 0.2226]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_4e-06\twd_0.0001\n",
      "tmb mae:0.594614127702363\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 09:47 metrics:0.595\n",
      "lr_9.973157730520429e-05\twd_0.0001\n",
      "tmb mae:0.6350867269984918\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 09:49 metrics:0.635\n",
      "lr_9.892377014066981e-05\twd_0.0001\n",
      "tmb mae:0.5737493715434892\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 09:50 metrics:0.574\n",
      "lr_9.758542201714877e-05\twd_0.0001\n",
      "tmb mae:0.5725867269984917\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 09:52 metrics:0.573\n",
      "lr_9.573119615674986e-05\twd_0.0001\n",
      "tmb mae:0.5942370537958773\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 09:54 metrics:0.594\n",
      "lr_9.338140784602127e-05\twd_0.0001\n",
      "tmb mae:0.5932943690296631\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 09:56 metrics:0.593\n",
      "lr_9.056180185742019e-05\twd_0.0001\n",
      "tmb mae:0.6211978381096028\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 09:58 metrics:0.621\n",
      "lr_8.730327038419821e-05\twd_0.0001\n",
      "tmb mae:0.6057378079436904\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 09:59 metrics:0.606\n",
      "lr_8.36415145790675e-05\twd_0.0001\n",
      "tmb mae:0.6481586224233283\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 10:01 metrics:0.648\n",
      "lr_7.961665340490073e-05\twd_0.0001\n",
      "tmb mae:0.6685834590246356\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 10:03 metrics:0.669\n",
      "lr_7.527278408297844e-05\twd_0.0001\n",
      "tmb mae:0.6564228255404727\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 10:05 metrics:0.656\n",
      "lr_7.06574989546047e-05\twd_0.0001\n",
      "tmb mae:0.6303104575163399\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 10:07 metrics:0.63\n",
      "lr_6.582136404945645e-05\twd_0.0001\n",
      "tmb mae:0.6301533433886375\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 10:08 metrics:0.63\n",
      "lr_6.081736507358081e-05\twd_0.0001\n",
      "tmb mae:0.5987305178481649\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 10:10 metrics:0.599\n",
      "lr_5.570032688691159e-05\twd_0.0001\n",
      "tmb mae:0.6553230266465562\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 10:12 metrics:0.655\n",
      "lr_5.052631283063098e-05\twd_0.0001\n",
      "tmb mae:0.6326985922574158\n",
      "model saved: tmb_fold_batch_size_8 2020-06-28 10:14 metrics:0.633\n",
      "lr_4.535201048547166e-05\twd_0.0001\n"
     ]
    }
   ],
   "source": [
    "lr =1e-4\n",
    "learn.fit_one_cycle(30,lr,pct_start=0, final_div=1e2, wd=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc_all</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.689133</td>\n",
       "      <td>3.391091</td>\n",
       "      <td>3.840389</td>\n",
       "      <td>05:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.898280</td>\n",
       "      <td>3.377273</td>\n",
       "      <td>3.828476</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.723307</td>\n",
       "      <td>3.344445</td>\n",
       "      <td>3.799606</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.609624</td>\n",
       "      <td>3.482748</td>\n",
       "      <td>3.935591</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.791501</td>\n",
       "      <td>3.393774</td>\n",
       "      <td>3.843707</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.659469</td>\n",
       "      <td>3.365929</td>\n",
       "      <td>3.817632</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.712448</td>\n",
       "      <td>3.373162</td>\n",
       "      <td>3.824712</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.563741</td>\n",
       "      <td>3.389116</td>\n",
       "      <td>3.839618</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.625133</td>\n",
       "      <td>3.398711</td>\n",
       "      <td>3.847990</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.794913</td>\n",
       "      <td>3.384864</td>\n",
       "      <td>3.836459</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.701408</td>\n",
       "      <td>3.392814</td>\n",
       "      <td>3.842792</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.568892</td>\n",
       "      <td>3.382020</td>\n",
       "      <td>3.832326</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.613697</td>\n",
       "      <td>3.378939</td>\n",
       "      <td>3.829509</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.477487</td>\n",
       "      <td>3.397910</td>\n",
       "      <td>3.847095</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.469640</td>\n",
       "      <td>3.350370</td>\n",
       "      <td>3.803255</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.634025</td>\n",
       "      <td>3.369100</td>\n",
       "      <td>3.820822</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.620918</td>\n",
       "      <td>3.417976</td>\n",
       "      <td>3.865018</td>\n",
       "      <td>04:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.720148</td>\n",
       "      <td>3.403138</td>\n",
       "      <td>3.851877</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.456443</td>\n",
       "      <td>3.377156</td>\n",
       "      <td>3.827689</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.853894</td>\n",
       "      <td>3.350862</td>\n",
       "      <td>3.804543</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.616180</td>\n",
       "      <td>3.367764</td>\n",
       "      <td>3.819252</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.499483</td>\n",
       "      <td>3.399429</td>\n",
       "      <td>3.848543</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.573730</td>\n",
       "      <td>3.412355</td>\n",
       "      <td>3.859242</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.638896</td>\n",
       "      <td>3.353860</td>\n",
       "      <td>3.806183</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.554863</td>\n",
       "      <td>3.367144</td>\n",
       "      <td>3.819207</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.832214</td>\n",
       "      <td>3.358302</td>\n",
       "      <td>3.810991</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.605481</td>\n",
       "      <td>3.364306</td>\n",
       "      <td>3.814981</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.778757</td>\n",
       "      <td>3.388604</td>\n",
       "      <td>3.839031</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.602760</td>\n",
       "      <td>3.385506</td>\n",
       "      <td>3.836196</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.585331</td>\n",
       "      <td>3.395292</td>\n",
       "      <td>3.845058</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.565812</td>\n",
       "      <td>3.391773</td>\n",
       "      <td>3.841924</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.431694</td>\n",
       "      <td>3.369612</td>\n",
       "      <td>3.821113</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.756041</td>\n",
       "      <td>3.372780</td>\n",
       "      <td>3.824561</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.838050</td>\n",
       "      <td>3.393034</td>\n",
       "      <td>3.843731</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.743479</td>\n",
       "      <td>3.369883</td>\n",
       "      <td>3.822282</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.509880</td>\n",
       "      <td>3.387217</td>\n",
       "      <td>3.837970</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.666371</td>\n",
       "      <td>3.386494</td>\n",
       "      <td>3.836777</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.585065</td>\n",
       "      <td>3.401177</td>\n",
       "      <td>3.850089</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.791329</td>\n",
       "      <td>3.358324</td>\n",
       "      <td>3.810961</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.406575</td>\n",
       "      <td>3.408738</td>\n",
       "      <td>3.857387</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.559174</td>\n",
       "      <td>3.481379</td>\n",
       "      <td>3.934359</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.739239</td>\n",
       "      <td>3.373172</td>\n",
       "      <td>3.824379</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.630832</td>\n",
       "      <td>3.371200</td>\n",
       "      <td>3.822474</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.545025</td>\n",
       "      <td>3.413990</td>\n",
       "      <td>3.863182</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.770638</td>\n",
       "      <td>3.343654</td>\n",
       "      <td>3.798181</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.509524</td>\n",
       "      <td>3.367580</td>\n",
       "      <td>3.819429</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.995885</td>\n",
       "      <td>3.390913</td>\n",
       "      <td>3.841466</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.651491</td>\n",
       "      <td>3.392790</td>\n",
       "      <td>3.842877</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.718194</td>\n",
       "      <td>3.387147</td>\n",
       "      <td>3.837528</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.715524</td>\n",
       "      <td>3.364457</td>\n",
       "      <td>3.816164</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.719651</td>\n",
       "      <td>3.392365</td>\n",
       "      <td>3.842410</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>4.002738</td>\n",
       "      <td>3.380760</td>\n",
       "      <td>3.831835</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.611630</td>\n",
       "      <td>3.363991</td>\n",
       "      <td>3.816328</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.715102</td>\n",
       "      <td>3.351234</td>\n",
       "      <td>3.804551</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.497331</td>\n",
       "      <td>3.390246</td>\n",
       "      <td>3.841009</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.325482</td>\n",
       "      <td>3.386712</td>\n",
       "      <td>3.837334</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.489900</td>\n",
       "      <td>3.401653</td>\n",
       "      <td>3.850802</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.769395</td>\n",
       "      <td>3.355264</td>\n",
       "      <td>3.807784</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.571151</td>\n",
       "      <td>3.369192</td>\n",
       "      <td>3.820576</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.655972</td>\n",
       "      <td>3.371642</td>\n",
       "      <td>3.822831</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.532961</td>\n",
       "      <td>3.385894</td>\n",
       "      <td>3.836240</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.548380</td>\n",
       "      <td>3.364655</td>\n",
       "      <td>3.816616</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.684428</td>\n",
       "      <td>3.365568</td>\n",
       "      <td>3.817296</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.945106</td>\n",
       "      <td>3.371246</td>\n",
       "      <td>3.822837</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.314912</td>\n",
       "      <td>3.382254</td>\n",
       "      <td>3.832902</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.667201</td>\n",
       "      <td>3.373578</td>\n",
       "      <td>3.824985</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.844032</td>\n",
       "      <td>3.386675</td>\n",
       "      <td>3.837026</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.583059</td>\n",
       "      <td>3.391451</td>\n",
       "      <td>3.841529</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.874274</td>\n",
       "      <td>3.369080</td>\n",
       "      <td>3.820585</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.741682</td>\n",
       "      <td>3.373494</td>\n",
       "      <td>3.825090</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.694599</td>\n",
       "      <td>3.383169</td>\n",
       "      <td>3.833871</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.679887</td>\n",
       "      <td>3.374096</td>\n",
       "      <td>3.825090</td>\n",
       "      <td>04:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.617518</td>\n",
       "      <td>3.379215</td>\n",
       "      <td>3.829796</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.555717</td>\n",
       "      <td>3.381081</td>\n",
       "      <td>3.831607</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>3.485005</td>\n",
       "      <td>3.381715</td>\n",
       "      <td>3.832486</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.633692</td>\n",
       "      <td>3.382098</td>\n",
       "      <td>3.832792</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.523747</td>\n",
       "      <td>3.380257</td>\n",
       "      <td>3.830860</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.661613</td>\n",
       "      <td>3.379835</td>\n",
       "      <td>3.830872</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.588841</td>\n",
       "      <td>3.378539</td>\n",
       "      <td>3.829572</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.745084</td>\n",
       "      <td>3.375870</td>\n",
       "      <td>3.826918</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.708345</td>\n",
       "      <td>3.373792</td>\n",
       "      <td>3.824948</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.601877</td>\n",
       "      <td>3.375268</td>\n",
       "      <td>3.826506</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>3.591219</td>\n",
       "      <td>3.375160</td>\n",
       "      <td>3.826070</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>3.532698</td>\n",
       "      <td>3.376660</td>\n",
       "      <td>3.827481</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>3.575459</td>\n",
       "      <td>3.379198</td>\n",
       "      <td>3.830070</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.744305</td>\n",
       "      <td>3.377431</td>\n",
       "      <td>3.828441</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>3.606774</td>\n",
       "      <td>3.376295</td>\n",
       "      <td>3.827191</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.864656</td>\n",
       "      <td>3.377205</td>\n",
       "      <td>3.828125</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.650979</td>\n",
       "      <td>3.375884</td>\n",
       "      <td>3.826877</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.980999</td>\n",
       "      <td>3.376534</td>\n",
       "      <td>3.827520</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.659247</td>\n",
       "      <td>3.376388</td>\n",
       "      <td>3.827379</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.665514</td>\n",
       "      <td>3.376845</td>\n",
       "      <td>3.827861</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>3.897869</td>\n",
       "      <td>3.377556</td>\n",
       "      <td>3.828505</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.915767</td>\n",
       "      <td>3.377634</td>\n",
       "      <td>3.828624</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>3.607958</td>\n",
       "      <td>3.377148</td>\n",
       "      <td>3.828044</td>\n",
       "      <td>04:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.893520</td>\n",
       "      <td>3.377295</td>\n",
       "      <td>3.828233</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>3.531406</td>\n",
       "      <td>3.377188</td>\n",
       "      <td>3.828121</td>\n",
       "      <td>04:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.662236</td>\n",
       "      <td>3.376998</td>\n",
       "      <td>3.827987</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>3.602268</td>\n",
       "      <td>3.377142</td>\n",
       "      <td>3.828012</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>3.701151</td>\n",
       "      <td>3.377052</td>\n",
       "      <td>3.827892</td>\n",
       "      <td>04:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_4e-05\twd_0.0001\n",
      "tmb mae:3.840388774871826\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 22:28 metrics:3.84\n",
      "lr_0.0009997555594960068\twd_0.0001\n",
      "tmb mae:3.8284761905670166\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 22:33 metrics:3.83\n",
      "lr_0.0009990184182396022\twd_0.0001\n",
      "tmb mae:3.7996063232421875\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 22:37 metrics:3.8\n",
      "lr_0.000997789299491253\twd_0.0001\n",
      "tmb mae:3.935591459274292\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 22:42 metrics:3.94\n",
      "lr_0.000996069416242771\twd_0.0001\n",
      "tmb mae:3.8437066078186035\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 22:47 metrics:3.84\n",
      "lr_0.0009938604658112778\twd_0.0001\n",
      "tmb mae:3.817631721496582\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 22:52 metrics:3.82\n",
      "lr_0.00099116462816416\twd_0.0001\n",
      "tmb mae:3.824711799621582\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 22:56 metrics:3.82\n",
      "lr_0.0009879845637677024\twd_0.0001\n",
      "tmb mae:3.8396177291870117\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:01 metrics:3.84\n",
      "lr_0.0009843234109615307\twd_0.0001\n",
      "tmb mae:3.847990036010742\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:06 metrics:3.85\n",
      "lr_0.0009801847828614485\twd_0.0001\n",
      "tmb mae:3.836459159851074\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:11 metrics:3.84\n",
      "lr_0.0009755727637937278\twd_0.0001\n",
      "tmb mae:3.84279203414917\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:15 metrics:3.84\n",
      "lr_0.0009704919052643722\twd_0.0001\n",
      "tmb mae:3.8323261737823486\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:20 metrics:3.83\n",
      "lr_0.0009649472214673313\twd_0.0001\n",
      "tmb mae:3.8295085430145264\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:25 metrics:3.83\n",
      "lr_0.0009589441843360946\twd_0.0001\n",
      "tmb mae:3.847094774246216\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:30 metrics:3.85\n",
      "lr_0.0009524887181435552\twd_0.0001\n",
      "tmb mae:3.803255319595337\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:34 metrics:3.8\n",
      "lr_0.0009455871936554674\twd_0.0001\n",
      "tmb mae:3.8208224773406982\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:39 metrics:3.82\n",
      "lr_0.00093824642184327\twd_0.0001\n",
      "tmb mae:3.865018367767334\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:44 metrics:3.87\n",
      "lr_0.000930473647162479\twd_0.0001\n",
      "tmb mae:3.851877450942993\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:49 metrics:3.85\n",
      "lr_0.0009222765404032863\twd_0.0001\n",
      "tmb mae:3.8276889324188232\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:53 metrics:3.83\n",
      "lr_0.0009136631911204144\twd_0.0001\n",
      "tmb mae:3.8045427799224854\n",
      "model saved: tmb_fold_batch_size_8 2020-06-18 23:58 metrics:3.8\n",
      "lr_0.0009046420996497031\twd_0.0001\n",
      "tmb mae:3.819251775741577\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:03 metrics:3.82\n",
      "lr_0.0008952221687193046\twd_0.0001\n",
      "tmb mae:3.848543405532837\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:08 metrics:3.85\n",
      "lr_0.0008854126946637652\twd_0.0001\n",
      "tmb mae:3.8592422008514404\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:13 metrics:3.86\n",
      "lr_0.0008752233582496652\twd_0.0001\n",
      "tmb mae:3.806182861328125\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:17 metrics:3.81\n",
      "lr_0.0008646642151218725\twd_0.0001\n",
      "tmb mae:3.819207191467285\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:22 metrics:3.82\n",
      "lr_0.0008537456858798327\twd_0.0001\n",
      "tmb mae:3.8109912872314453\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:27 metrics:3.81\n",
      "lr_0.0008424785457936981\twd_0.0001\n",
      "tmb mae:3.81498122215271\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:32 metrics:3.81\n",
      "lr_0.0008308739141704353\twd_0.0001\n",
      "tmb mae:3.839031219482422\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:36 metrics:3.84\n",
      "lr_0.0008189432433804123\twd_0.0001\n",
      "tmb mae:3.8361964225769043\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:41 metrics:3.84\n",
      "lr_0.0008066983075552915\twd_0.0001\n",
      "tmb mae:3.8450584411621094\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:46 metrics:3.85\n",
      "lr_0.0007941511909683832\twd_0.0001\n",
      "tmb mae:3.8419244289398193\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:51 metrics:3.84\n",
      "lr_0.0007813142761089271\twd_0.0001\n",
      "tmb mae:3.821113348007202\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 00:55 metrics:3.82\n",
      "lr_0.0007682002314620701\twd_0.0001\n",
      "tmb mae:3.824561357498169\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:00 metrics:3.82\n",
      "lr_0.0007548219990066007\twd_0.0001\n",
      "tmb mae:3.8437306880950928\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:05 metrics:3.84\n",
      "lr_0.0007411927814427785\twd_0.0001\n",
      "tmb mae:3.8222815990448\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:10 metrics:3.82\n",
      "lr_0.0007273260291628641\twd_0.0001\n",
      "tmb mae:3.8379695415496826\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:14 metrics:3.84\n",
      "lr_0.0007132354269772042\twd_0.0001\n",
      "tmb mae:3.8367772102355957\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:19 metrics:3.84\n",
      "lr_0.0006989348806089774\twd_0.0001\n",
      "tmb mae:3.8500893115997314\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:24 metrics:3.85\n",
      "lr_0.0006844385029709228\twd_0.0001\n",
      "tmb mae:3.8109607696533203\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:29 metrics:3.81\n",
      "lr_0.0006697606002376007\twd_0.0001\n",
      "tmb mae:3.8573873043060303\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:34 metrics:3.86\n",
      "lr_0.0006549156577269239\twd_0.0001\n",
      "tmb mae:3.934359073638916\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:38 metrics:3.93\n",
      "lr_0.0006399183256048988\twd_0.0001\n",
      "tmb mae:3.824378728866577\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:43 metrics:3.82\n",
      "lr_0.0006247834044276801\twd_0.0001\n",
      "tmb mae:3.822474479675293\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:48 metrics:3.82\n",
      "lr_0.0006095258305352086\twd_0.0001\n",
      "tmb mae:3.8631815910339355\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:53 metrics:3.86\n",
      "lr_0.000594160661310847\twd_0.0001\n",
      "tmb mae:3.7981812953948975\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 01:57 metrics:3.8\n",
      "lr_0.0005787030603215601\twd_0.0001\n",
      "tmb mae:3.8194291591644287\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:02 metrics:3.82\n",
      "lr_0.0005631682823533058\twd_0.0001\n",
      "tmb mae:3.841466188430786\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:07 metrics:3.84\n",
      "lr_0.0005475716583564023\twd_0.0001\n",
      "tmb mae:3.84287691116333\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:12 metrics:3.84\n",
      "lr_0.0005319285803157311\twd_0.0001\n",
      "tmb mae:3.8375282287597656\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:16 metrics:3.84\n",
      "lr_0.0005162544860607067\twd_0.0001\n",
      "tmb mae:3.816164493560791\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:21 metrics:3.82\n",
      "lr_0.0005005648440300026\twd_0.0001\n",
      "tmb mae:3.84240984916687\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:26 metrics:3.84\n",
      "lr_0.00048487513800606963\twd_0.0001\n",
      "tmb mae:3.8318350315093994\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:31 metrics:3.83\n",
      "lr_0.0004692008518345122\twd_0.0001\n",
      "tmb mae:3.816328287124634\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:36 metrics:3.82\n",
      "lr_0.00045355745414340213\twd_0.0001\n",
      "tmb mae:3.8045506477355957\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:40 metrics:3.8\n",
      "lr_0.0004379603830776103\twd_0.0001\n",
      "tmb mae:3.8410086631774902\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:45 metrics:3.84\n",
      "lr_0.00042242503106322153\twd_0.0001\n",
      "tmb mae:3.837333917617798\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:50 metrics:3.84\n",
      "lr_0.0004069667296170679\twd_0.0001\n",
      "tmb mae:3.850802421569824\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:55 metrics:3.85\n",
      "lr_0.00039160073421637377\twd_0.0001\n",
      "tmb mae:3.807783603668213\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 02:59 metrics:3.81\n",
      "lr_0.0003763422092434404\twd_0.0001\n",
      "tmb mae:3.8205764293670654\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:04 metrics:3.82\n",
      "lr_0.00036120621302023197\twd_0.0001\n",
      "tmb mae:3.8228306770324707\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:09 metrics:3.82\n",
      "lr_0.00034620768294762983\twd_0.0001\n",
      "tmb mae:3.8362395763397217\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:14 metrics:3.84\n",
      "lr_0.0003313614207640212\twd_0.0001\n",
      "tmb mae:3.8166158199310303\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:18 metrics:3.82\n",
      "lr_0.0003166820779377706\twd_0.0001\n",
      "tmb mae:3.817296028137207\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:23 metrics:3.82\n",
      "lr_0.0003021841412079885\twd_0.0001\n",
      "tmb mae:3.8228371143341064\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:28 metrics:3.82\n",
      "lr_0.0002878819182878703\twd_0.0001\n",
      "tmb mae:3.832902431488037\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:33 metrics:3.83\n",
      "lr_0.00027378952374470996\twd_0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmb mae:3.8249852657318115\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:37 metrics:3.82\n",
      "lr_0.000259920865070526\twd_0.0001\n",
      "tmb mae:3.8370258808135986\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:42 metrics:3.84\n",
      "lr_0.0002462896289570466\twd_0.0001\n",
      "tmb mae:3.841529369354248\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:47 metrics:3.84\n",
      "lr_0.00023290926778859658\twd_0.0001\n",
      "tmb mae:3.820585250854492\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:52 metrics:3.82\n",
      "lr_0.0002197929863662184\twd_0.0001\n",
      "tmb mae:3.825089931488037\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 03:57 metrics:3.83\n",
      "lr_0.00020695372887612852\twd_0.0001\n",
      "tmb mae:3.8338708877563477\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:01 metrics:3.83\n",
      "lr_0.00019440416611536692\twd_0.0001\n",
      "tmb mae:3.8250904083251953\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:06 metrics:3.83\n",
      "lr_0.0001821566829872517\twd_0.0001\n",
      "tmb mae:3.829796075820923\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:11 metrics:3.83\n",
      "lr_0.0001702233662789735\twd_0.0001\n",
      "tmb mae:3.831606864929199\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:16 metrics:3.83\n",
      "lr_0.00015861599273339706\twd_0.0001\n",
      "tmb mae:3.832486152648926\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:20 metrics:3.83\n",
      "lr_0.000147346017426836\twd_0.0001\n",
      "tmb mae:3.832792282104492\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:25 metrics:3.83\n",
      "lr_0.00013642456246427782\twd_0.0001\n",
      "tmb mae:3.830859899520874\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:30 metrics:3.83\n",
      "lr_0.00012586240600320693\twd_0.0001\n",
      "tmb mae:3.830872058868408\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:35 metrics:3.83\n",
      "lr_0.0001156699716168662\twd_0.0001\n",
      "tmb mae:3.8295724391937256\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:40 metrics:3.83\n",
      "lr_0.00010585731800744713\twd_0.0001\n",
      "tmb mae:3.826918363571167\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:44 metrics:3.83\n",
      "lr_9.643412907936638e-05\twd_0.0001\n",
      "tmb mae:3.8249475955963135\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:49 metrics:3.82\n",
      "lr_8.740970438242045e-05\twd_0.0001\n",
      "tmb mae:3.826505661010742\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:54 metrics:3.83\n",
      "lr_7.87929499342534e-05\twd_0.0001\n",
      "tmb mae:3.8260698318481445\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 04:59 metrics:3.83\n",
      "lr_7.059236943119189e-05\twd_0.0001\n",
      "tmb mae:3.8274810314178467\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:03 metrics:3.83\n",
      "lr_6.281605585612471e-05\twd_0.0001\n",
      "tmb mae:3.8300700187683105\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:08 metrics:3.83\n",
      "lr_5.547168349170447e-05\twd_0.0001\n",
      "tmb mae:3.8284411430358887\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:13 metrics:3.83\n",
      "lr_4.856650034675828e-05\twd_0.0001\n",
      "tmb mae:3.827190637588501\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:18 metrics:3.83\n",
      "lr_4.2107321003377154e-05\twd_0.0001\n",
      "tmb mae:3.828125\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:23 metrics:3.83\n",
      "lr_3.610051989174685e-05\twd_0.0001\n",
      "tmb mae:3.8268773555755615\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:27 metrics:3.83\n",
      "lr_3.055202499935464e-05\twd_0.0001\n",
      "tmb mae:3.8275203704833984\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:32 metrics:3.83\n",
      "lr_2.5467312020781535e-05\twd_0.0001\n",
      "tmb mae:3.8273792266845703\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:37 metrics:3.83\n",
      "lr_2.0851398953852404e-05\twd_0.0001\n",
      "tmb mae:3.8278608322143555\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:42 metrics:3.83\n",
      "lr_1.6708841147478506e-05\twd_0.0001\n",
      "tmb mae:3.828504800796509\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:46 metrics:3.83\n",
      "lr_1.3043726806077577e-05\twd_0.0001\n",
      "tmb mae:3.8286237716674805\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:51 metrics:3.83\n",
      "lr_9.859672955009893e-06\twd_0.0001\n",
      "tmb mae:3.8280441761016846\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 05:56 metrics:3.83\n",
      "lr_7.159821871011145e-06\twd_0.0001\n",
      "tmb mae:3.828233242034912\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 06:01 metrics:3.83\n",
      "lr_4.946837981144202e-06\twd_0.0001\n",
      "tmb mae:3.8281209468841553\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 06:05 metrics:3.83\n",
      "lr_3.222905233331565e-06\twd_0.0001\n",
      "tmb mae:3.8279869556427\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 06:10 metrics:3.83\n",
      "lr_1.98972494106238e-06\twd_0.0001\n",
      "tmb mae:3.8280115127563477\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 06:15 metrics:3.83\n",
      "lr_1.2485141044015016e-06\twd_0.0001\n",
      "tmb mae:3.8278918266296387\n",
      "model saved: tmb_fold_batch_size_8 2020-06-19 06:20 metrics:3.83\n"
     ]
    }
   ],
   "source": [
    "lr =1e-3\n",
    "learn.fit_one_cycle(100,lr,pct_start=0, final_div=1e3, wd=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## patch to whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tcga_whole_dataset():\n",
    "    def __init__(self, learn, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset =  dataset\n",
    "        self.df = dataset.df\n",
    "        self.fns = list(set(self.df['FILE_NAME'].tolist()))\n",
    "        self.model = learn.model.eval()\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        df = self.df[self.df.FILE_NAME == self.fns[i]]\n",
    "        n = df.shape[0]\n",
    "        features_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for i_patch, row in df.iterrows():\n",
    "                img, label = self.dataset[i_patch]\n",
    "                img = torch.tensor(img).unsqueeze(0).cuda()\n",
    "                feature = self.model.enet.extract_features(img)\n",
    "                features_sum += feature\n",
    "            tmb = np.float32(row.TMB_38Mb)\n",
    "            feature_average = features_sum / n\n",
    "        return feature_average.squeeze(0), (label.argmax().astype(np.long), tmb)\n",
    "    def __len__(self):\n",
    "#         return 8\n",
    "        return len(self.fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_model(nn.Module):\n",
    "    def __init__(self, num_input_features, ):\n",
    "        super().__init__()\n",
    "        self.out_put_dim = num_input_features//2\n",
    "        self.m = nn.Sequential(OrderedDict(\n",
    "        {'norm1': nn.BatchNorm2d(num_input_features),\n",
    "        'relu1': nn.ReLU(inplace=False),\n",
    "        'conv1': nn.Conv2d(num_input_features, self.out_put_dim, kernel_size=3, stride=1,\n",
    "                                           bias=False),\n",
    "        'norm2': nn.BatchNorm2d(self.out_put_dim),\n",
    "        'relu2': nn.ReLU(inplace=False),\n",
    "        'conv2': nn.Conv2d(self.out_put_dim, self.out_put_dim, kernel_size=3, stride=1,\n",
    "                                           bias=False),\n",
    "        'norm3': nn.BatchNorm2d(self.out_put_dim),\n",
    "        'relu3': nn.ReLU(inplace=False),\n",
    "        'conv3': nn.Conv2d(self.out_put_dim, self.out_put_dim, kernel_size=3, stride=1,\n",
    "                                           bias=False)}))\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(num_input_features//2, 3)\n",
    "        self.tmb_fc = nn.Linear(num_input_features//2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.m(x)\n",
    "        x = self.pooling(x)\n",
    "        x = x.view(x.shape[0], self.out_put_dim)\n",
    "        x1 = self.fc(x)\n",
    "        x2 = self.tmb_fc(x)\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=DataBunch;\n",
       "\n",
       "Train: <dataset.tcga_dataset.TCGADataset object at 0x7fb197b421d0>;\n",
       "\n",
       "Valid: <dataset.tcga_dataset.TCGADataset object at 0x7fb18d6ecb00>;\n",
       "\n",
       "Test: <dataset.tcga_dataset.TCGADataset object at 0x7fb18d586a90>, model=enetv2(\n",
       "  (enet): EfficientNet(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (3): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (4): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (6): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (7): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (9): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (10): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (11): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (12): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (13): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (14): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (15): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (_swish): MemoryEfficientSwish()\n",
       "  )\n",
       "  (myfc): Linear(in_features=1280, out_features=3, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=lsr_cross_entropy(\n",
       "  (log_softmax): LogSoftmax()\n",
       "), metrics=[acc_all], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False), <class '__main__.LearnCallbacks'>], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  (1): ParameterModule()\n",
       "  (2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (3): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (4): ParameterModule()\n",
       "  (5): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (6): Identity()\n",
       "  (7): ParameterModule()\n",
       "  (8): ParameterModule()\n",
       "  (9): Identity()\n",
       "  (10): ParameterModule()\n",
       "  (11): ParameterModule()\n",
       "  (12): Identity()\n",
       "  (13): ParameterModule()\n",
       "  (14): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (15): MemoryEfficientSwish()\n",
       "  (16): Identity()\n",
       "  (17): ParameterModule()\n",
       "  (18): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (19): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  (20): ParameterModule()\n",
       "  (21): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (22): Identity()\n",
       "  (23): ParameterModule()\n",
       "  (24): ParameterModule()\n",
       "  (25): Identity()\n",
       "  (26): ParameterModule()\n",
       "  (27): ParameterModule()\n",
       "  (28): Identity()\n",
       "  (29): ParameterModule()\n",
       "  (30): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (31): MemoryEfficientSwish()\n",
       "  (32): Identity()\n",
       "  (33): ParameterModule()\n",
       "  (34): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (35): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (36): ParameterModule()\n",
       "  (37): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (38): Identity()\n",
       "  (39): ParameterModule()\n",
       "  (40): ParameterModule()\n",
       "  (41): Identity()\n",
       "  (42): ParameterModule()\n",
       "  (43): ParameterModule()\n",
       "  (44): Identity()\n",
       "  (45): ParameterModule()\n",
       "  (46): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (47): MemoryEfficientSwish()\n",
       "  (48): Identity()\n",
       "  (49): ParameterModule()\n",
       "  (50): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (51): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "  (52): ParameterModule()\n",
       "  (53): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (54): Identity()\n",
       "  (55): ParameterModule()\n",
       "  (56): ParameterModule()\n",
       "  (57): Identity()\n",
       "  (58): ParameterModule()\n",
       "  (59): ParameterModule()\n",
       "  (60): Identity()\n",
       "  (61): ParameterModule()\n",
       "  (62): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (63): MemoryEfficientSwish()\n",
       "  (64): Identity()\n",
       "  (65): ParameterModule()\n",
       "  (66): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (67): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "  (68): ParameterModule()\n",
       "  (69): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (70): Identity()\n",
       "  (71): ParameterModule()\n",
       "  (72): ParameterModule()\n",
       "  (73): Identity()\n",
       "  (74): ParameterModule()\n",
       "  (75): ParameterModule()\n",
       "  (76): Identity()\n",
       "  (77): ParameterModule()\n",
       "  (78): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (79): MemoryEfficientSwish()\n",
       "  (80): Identity()\n",
       "  (81): ParameterModule()\n",
       "  (82): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (83): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  (84): ParameterModule()\n",
       "  (85): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (86): Identity()\n",
       "  (87): ParameterModule()\n",
       "  (88): ParameterModule()\n",
       "  (89): Identity()\n",
       "  (90): ParameterModule()\n",
       "  (91): ParameterModule()\n",
       "  (92): Identity()\n",
       "  (93): ParameterModule()\n",
       "  (94): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (95): MemoryEfficientSwish()\n",
       "  (96): Identity()\n",
       "  (97): ParameterModule()\n",
       "  (98): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (99): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (100): ParameterModule()\n",
       "  (101): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (102): Identity()\n",
       "  (103): ParameterModule()\n",
       "  (104): ParameterModule()\n",
       "  (105): Identity()\n",
       "  (106): ParameterModule()\n",
       "  (107): ParameterModule()\n",
       "  (108): Identity()\n",
       "  (109): ParameterModule()\n",
       "  (110): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (111): MemoryEfficientSwish()\n",
       "  (112): Identity()\n",
       "  (113): ParameterModule()\n",
       "  (114): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (115): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (116): ParameterModule()\n",
       "  (117): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (118): Identity()\n",
       "  (119): ParameterModule()\n",
       "  (120): ParameterModule()\n",
       "  (121): Identity()\n",
       "  (122): ParameterModule()\n",
       "  (123): ParameterModule()\n",
       "  (124): Identity()\n",
       "  (125): ParameterModule()\n",
       "  (126): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (127): MemoryEfficientSwish()\n",
       "  (128): Identity()\n",
       "  (129): ParameterModule()\n",
       "  (130): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (131): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "  (132): ParameterModule()\n",
       "  (133): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (134): Identity()\n",
       "  (135): ParameterModule()\n",
       "  (136): ParameterModule()\n",
       "  (137): Identity()\n",
       "  (138): ParameterModule()\n",
       "  (139): ParameterModule()\n",
       "  (140): Identity()\n",
       "  (141): ParameterModule()\n",
       "  (142): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (143): MemoryEfficientSwish()\n",
       "  (144): Identity()\n",
       "  (145): ParameterModule()\n",
       "  (146): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (147): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "  (148): ParameterModule()\n",
       "  (149): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (150): Identity()\n",
       "  (151): ParameterModule()\n",
       "  (152): ParameterModule()\n",
       "  (153): Identity()\n",
       "  (154): ParameterModule()\n",
       "  (155): ParameterModule()\n",
       "  (156): Identity()\n",
       "  (157): ParameterModule()\n",
       "  (158): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (159): MemoryEfficientSwish()\n",
       "  (160): Identity()\n",
       "  (161): ParameterModule()\n",
       "  (162): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (163): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "  (164): ParameterModule()\n",
       "  (165): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (166): Identity()\n",
       "  (167): ParameterModule()\n",
       "  (168): ParameterModule()\n",
       "  (169): Identity()\n",
       "  (170): ParameterModule()\n",
       "  (171): ParameterModule()\n",
       "  (172): Identity()\n",
       "  (173): ParameterModule()\n",
       "  (174): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (175): MemoryEfficientSwish()\n",
       "  (176): Identity()\n",
       "  (177): ParameterModule()\n",
       "  (178): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (179): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "  (180): ParameterModule()\n",
       "  (181): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (182): Identity()\n",
       "  (183): ParameterModule()\n",
       "  (184): ParameterModule()\n",
       "  (185): Identity()\n",
       "  (186): ParameterModule()\n",
       "  (187): ParameterModule()\n",
       "  (188): Identity()\n",
       "  (189): ParameterModule()\n",
       "  (190): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (191): MemoryEfficientSwish()\n",
       "  (192): Identity()\n",
       "  (193): ParameterModule()\n",
       "  (194): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (195): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "  (196): ParameterModule()\n",
       "  (197): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (198): Identity()\n",
       "  (199): ParameterModule()\n",
       "  (200): ParameterModule()\n",
       "  (201): Identity()\n",
       "  (202): ParameterModule()\n",
       "  (203): ParameterModule()\n",
       "  (204): Identity()\n",
       "  (205): ParameterModule()\n",
       "  (206): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (207): MemoryEfficientSwish()\n",
       "  (208): Identity()\n",
       "  (209): ParameterModule()\n",
       "  (210): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (211): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "  (212): ParameterModule()\n",
       "  (213): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (214): Identity()\n",
       "  (215): ParameterModule()\n",
       "  (216): ParameterModule()\n",
       "  (217): Identity()\n",
       "  (218): ParameterModule()\n",
       "  (219): ParameterModule()\n",
       "  (220): Identity()\n",
       "  (221): ParameterModule()\n",
       "  (222): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (223): MemoryEfficientSwish()\n",
       "  (224): Identity()\n",
       "  (225): ParameterModule()\n",
       "  (226): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (227): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "  (228): ParameterModule()\n",
       "  (229): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (230): Identity()\n",
       "  (231): ParameterModule()\n",
       "  (232): ParameterModule()\n",
       "  (233): Identity()\n",
       "  (234): ParameterModule()\n",
       "  (235): ParameterModule()\n",
       "  (236): Identity()\n",
       "  (237): ParameterModule()\n",
       "  (238): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (239): MemoryEfficientSwish()\n",
       "  (240): Identity()\n",
       "  (241): ParameterModule()\n",
       "  (242): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (243): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "  (244): ParameterModule()\n",
       "  (245): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (246): Identity()\n",
       "  (247): ParameterModule()\n",
       "  (248): ParameterModule()\n",
       "  (249): Identity()\n",
       "  (250): ParameterModule()\n",
       "  (251): ParameterModule()\n",
       "  (252): Identity()\n",
       "  (253): ParameterModule()\n",
       "  (254): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (255): MemoryEfficientSwish()\n",
       "  (256): Identity()\n",
       "  (257): ParameterModule()\n",
       "  (258): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (259): AdaptiveAvgPool2d(output_size=1)\n",
       "  (260): Dropout(p=0.2, inplace=False)\n",
       "  (261): MemoryEfficientSwish()\n",
       "  (262): Linear(in_features=1280, out_features=3, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ds = tcga_whole_dataset(learn, data.valid_ds)\n",
    "model_whole = simple_model(1280).cuda()\n",
    "batch_size_whole = 8\n",
    "learn.load('/mnt/data/tcga/models/bigpic_True_label_3fold_batch_size_8 2020-06-17 10:40 acc:0.813')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_whole = tcga_whole_dataset(learn, data.train_ds)\n",
    "dl_train_whole = DataLoader(dataset_train_whole, batch_size=batch_size_whole, shuffle=True, num_workers=0, drop_last=True,pin_memory=False)\n",
    "\n",
    "dataset_val_whole = tcga_whole_dataset(learn, data.valid_ds)\n",
    "dl_val_whole = DataLoader(dataset_val_whole, batch_size=batch_size_whole, shuffle=False, num_workers=0, pin_memory=False,drop_last=False)\n",
    "\n",
    "data_whole = DataBunch(train_dl=dl_train_whole, valid_dl=dl_val_whole, device='cuda', no_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class acc_whole(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esp = 1e-8\n",
    "\n",
    "    def on_epoch_begin(self,**kwargs):\n",
    "        self.xo = []\n",
    "        self.yo = []\n",
    "        self.x_t = []\n",
    "        self.y_t = []\n",
    "        \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        self.x_t.append(last_output[1].detach().flatten())\n",
    "        self.y_t.append(last_target[1].flatten())  \n",
    "        self.xo.append(last_output[0].argmax(-1).detach().flatten())\n",
    "        self.yo.append(last_target[0].flatten())\n",
    "#         pred = logits.sigmoid().sum(1).detach().round()\n",
    "#         print(self.xo, self.yo)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, last_metrics, **kwargs):        \n",
    "        self.last_metrics = last_metrics\n",
    "        self.y = np.array(torch.cat(self.xo).detach().cpu().numpy())\n",
    "        self.l = np.array(torch.cat(self.yo).detach().cpu().numpy())\n",
    "#         self.y_tmb = np.array(torch.cat(self.x_t).detach().cpu().numpy())\n",
    "#         self.l_tmb = np.array(torch.cat(self.y_t).detach().cpu().numpy())  \n",
    "        self.y_tmb = torch.cat(self.x_t).detach().cpu()\n",
    "        self.l_tmb = torch.cat(self.y_t).detach().cpu()\n",
    "        f1_tmb = mean_absolute_error(self.l_tmb[self.l_tmb>0], self.y_tmb[self.l_tmb>0])\n",
    "#         print((self.y_tmb>=0).sum(), (self.l_tmb>0).sum())\n",
    "        print(f'tmb mae:{f1_tmb}')\n",
    "        matrix = confusion_matrix(self.y, self.l)\n",
    "        print(matrix)\n",
    "        return {\"last_metrics\":self.last_metrics + [(self.y==self.l).sum()/self.y.shape[0]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_whole = Learner(data_whole, model_whole, loss_func=tmb_cls_loss(), callback_fns=[LearnCallbacks], metrics=[acc_whole()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='15', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      33.33% [5/15 2:16:29<4:32:58]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc_whole</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.434596</td>\n",
       "      <td>4.266017</td>\n",
       "      <td>0.795527</td>\n",
       "      <td>26:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.155612</td>\n",
       "      <td>3.937129</td>\n",
       "      <td>0.857827</td>\n",
       "      <td>27:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.524951</td>\n",
       "      <td>4.193159</td>\n",
       "      <td>0.856230</td>\n",
       "      <td>28:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.706205</td>\n",
       "      <td>4.411701</td>\n",
       "      <td>0.865815</td>\n",
       "      <td>26:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.173303</td>\n",
       "      <td>4.275243</td>\n",
       "      <td>0.861022</td>\n",
       "      <td>26:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='67' class='' max='246', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      27.24% [67/246 05:36<14:58 4.3835]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_0.00012\twd_0.001\n",
      "torch.Size([626]) torch.Size([626])\n",
      "tmb mae:3.9100501537323\n",
      "[[108  22   7]\n",
      " [  1 154  12]\n",
      " [ 12  74 236]]\n",
      "model saved: bigpic_True_label_3fold_batch_size_8 2020-06-18 14:44 acc:0.796\n",
      "lr_0.0004568960019086716\twd_0.001\n",
      "torch.Size([626]) torch.Size([626])\n",
      "tmb mae:3.8521065711975098\n",
      "[[108  10   5]\n",
      " [  6 220  41]\n",
      " [  7  20 209]]\n",
      "model saved: bigpic_True_label_3fold_batch_size_8 2020-06-18 15:12 acc:0.858\n",
      "lr_0.00130994662415962\twd_0.001\n",
      "torch.Size([626]) torch.Size([626])\n",
      "tmb mae:4.201976776123047\n",
      "[[110   7   4]\n",
      " [  2 192  17]\n",
      " [  9  51 234]]\n",
      "model saved: bigpic_True_label_3fold_batch_size_8 2020-06-18 15:41 acc:0.856\n",
      "lr_0.00228\twd_0.001\n",
      "torch.Size([626]) torch.Size([626])\n",
      "tmb mae:4.1128315925598145\n",
      "[[ 99   1   1]\n",
      " [  6 221  32]\n",
      " [ 16  28 222]]\n",
      "model saved: bigpic_True_label_3fold_batch_size_8 2020-06-18 16:08 acc:0.866\n",
      "lr_0.002913157373931708\twd_0.001\n",
      "torch.Size([626]) torch.Size([626])\n",
      "tmb mae:4.171572685241699\n",
      "[[109   4   6]\n",
      " [  4 201  20]\n",
      " [  8  45 229]]\n",
      "model saved: bigpic_True_label_3fold_batch_size_8 2020-06-18 16:34 acc:0.861\n",
      "lr_0.0029832463063527354\twd_0.001\n"
     ]
    }
   ],
   "source": [
    "learn_whole.fit_one_cycle(15, wd=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Your training dataloader is empty, can't train a model.\n        Use a smaller batch size (batch size=8 for 4 elements).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-082da638572e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_whole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
      "\u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;34m\"Fit the `model` on `data` and learn using `loss_func` and `opt`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     assert len(learn.data.train_dl) != 0, f\"\"\"Your training dataloader is empty, can't train a model.\n\u001b[0;32m---> 88\u001b[0;31m         Use a smaller batch size (batch size={learn.data.train_dl.batch_size} for {len(learn.data.train_dl.dataset)} elements).\"\"\"\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mcb_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallbackHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Your training dataloader is empty, can't train a model.\n        Use a smaller batch size (batch size=8 for 4 elements)."
     ]
    }
   ],
   "source": [
    "learn_whole.fit_one_cycle(15, wd=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8]) torch.Size([8])\n",
      "tensor(8) tensor(8)\n",
      "tmb mae:5.486178398132324\n",
      "[[0 0 0]\n",
      " [1 1 0]\n",
      " [2 0 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.0888753, 0.625]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_whole.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distile(LearnerCallback):\n",
    "    _order = 99\n",
    "    def __init__(self, learn):\n",
    "        super().__init__(learn)\n",
    "        self.learn = learn\n",
    "        self.df = learn.data.test_ds.df\n",
    "        self.data_dir = learn.data.test_ds.data_dir\n",
    "        \n",
    "    def on_epoch_begin(self,**kwargs):\n",
    "        self.xo = []\n",
    "        self.yo = []\n",
    "\n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        self.xo.append(last_output.softmax(-1).detach())\n",
    "        self.yo.append(last_target)\n",
    "#         pred = logits.sigmoid().sum(1).detach().round()\n",
    "#         print(self.xo[0].shape, self.yo[0].shape)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, last_metrics, **kwargs):        \n",
    "        self.y = np.array(torch.cat(self.xo, dim=0).detach().cpu().numpy())\n",
    "        self.l = np.array(torch.cat(self.yo, dim=0).detach().cpu().numpy())\n",
    "        new_label = (self.y + self.l) / 2\n",
    "        print(self.df.shape, new_label.shape)\n",
    "        self.df['new_label3'] = self.df['new_label']\n",
    "        self.df['new_label'] = new_label.tolist()\n",
    "        self.df.to_csv(os.path.join(self.data_dir, 'train_distile2.csv'), index=None, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 776    9  116]\n",
      " [  48 5610  543]\n",
      " [  30  282 5053]]\n",
      "(12467, 20) (12467, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37560260491476893, 0.9175423117028957]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distile = Distile(learn)\n",
    "learn.validate(dl=data.test_dl, metrics=[acc_all()], callbacks=[distile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 258    7   17]\n",
      " [ 194 1111  367]\n",
      " [ 116  209 1437]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5047397499641382, 0.7551130247578041]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.data = data\n",
    "learn.validate(dl=data.valid_dl, metrics=[acc_all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =5e-6\n",
    "learn.fit_one_cycle(20,lr,pct_start=0.1, final_div=1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.metrics = [metrics_callback(dataset_val.df)]\n",
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ori_label.pkl', 'wb') as f:\n",
    "    pickle.dump(dcp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
